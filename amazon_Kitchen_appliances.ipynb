{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# content\n",
    "- [web scraping](#web_scraping)\n",
    "- [data cleaning](#data_cleaning)\n",
    "- [data analysis](#data_analysis)\n",
    "- [conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re as re\n",
    "import os\n",
    "import psycopg2 as ps\n",
    "import sqlite3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## project scenario\n",
    "- we are a startup starting a business in the kitchen appliances industry, we need to know **our main competitors** , **the most succeful products in each category** , and the **characteristics and pricing of the most succefull products.**\n",
    "- the company is willing to produce products in 7 main categories: air fryers, food processors, espresso machines, blenders, mixers, toasters, and electric kettles.\n",
    "- in order to answer the above questions we will scrape amazon data about the products with +4 stars rating in each of these catogeries and analyze their data to give a reliable answer to each one of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# web_scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I create the main file which we will use in the data cleaning and analysis, we scrape each category by providing its link and scrape its products' details using the append_product function. <br>\n",
    "I commented the lines that will create a new file just for myself so that the file doesn't get deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "main_page='https://www.amazon.eg/s?i=home&bbn=21863949031&rh=n%3A18021933031%2Cn%3A21863792031%2Cn%3A21863827031%2Cn%3A21863949031%2Cn%3A26957429031%2Cp_72%3A21909187031&dc&fs=true&language=en&ds=v1%3AAU%2FON3FdtLe4umfE6xf%2FVGlQf3BC%2B4wHfqH7eQw3qI0&qid=1673216837&rnid=21863949031&ref=sr_nr_n_1'\n",
    "mains=[main_page]\n",
    "filename='amazonKitchenFullData.csv'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# creating the file and its headers\n",
    "header=['title','brand','category','price','date','rating', 'num_of_ratings','link']\n",
    "\n",
    "with open(filename,'w',newline='',encoding='UTF8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "titles=[]\n",
    "def append_links(url):\n",
    "    '''\n",
    "    this function takes the page link as argument and returns all the products links in that page. \n",
    "    '''\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(url,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    for link in soup2.find_all('a',attrs={\"class\": \"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"}):\n",
    "        links.append('https://www.amazon.eg'+link.get('href'))\n",
    "    for title in soup2.find_all('span',attrs={\"class\": \"a-size-medium a-color-base a-text-normal\"}):\n",
    "        titles.append(title.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,4): # the range is the number of pages\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(main_page,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    try:\n",
    "        main2=soup2.find('a',attrs={\"class\": \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"}).get('href')\n",
    "    except:\n",
    "        main2=main_page\n",
    "    if main2 in mains:\n",
    "        append_links(main_page)\n",
    "        break\n",
    "    else:\n",
    "        mains.append('https://www.amazon.eg'+main2)\n",
    "    append_links(main_page)\n",
    "    main_page='https://www.amazon.eg'+main2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the length and the links\n",
    "print(len(mains))\n",
    "print(mains[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titles[-5:])\n",
    "print(len(titles))\n",
    "print(links[-5:])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingLinks=[]\n",
    "def appendProduct(link):\n",
    "    '''\n",
    "    this function takes the product link as an argument and returns the title, the price, category, rating, number of ratings, date and the link of the product.\n",
    "    '''\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    title=titles[i]\n",
    "    try:\n",
    "        brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                brand='N/A'\n",
    "    try:\n",
    "        price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                price='N/A'\n",
    "    try:\n",
    "        num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                num_of_ratings='N/A'\n",
    "\n",
    "    today=datetime.date.today()\n",
    "    try:\n",
    "        rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "        if rating=='Previous page':\n",
    "            rating='N/A'\n",
    "        else:\n",
    "            rating=rating\n",
    "    except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "                if rating=='Previous page':\n",
    "                    rating='N/A'\n",
    "                else:\n",
    "                    rating=rating\n",
    "            except AttributeError:\n",
    "                rating='N/A'\n",
    "    if rating == 'N/A' and price == 'N/A':\n",
    "        missingLinks.append(links[i])\n",
    "    link=links[i]\n",
    "    category='air fryers'\n",
    "    data=[title,brand,category,price,today,rating, num_of_ratings,link]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over products\n",
    "i=0\n",
    "for link in links:\n",
    "    appendProduct(links[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start repeating the process for each category until we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page='https://www.amazon.eg/s?i=home&bbn=21863955031&rh=n%3A18021933031%2Cn%3A21863792031%2Cn%3A21863827031%2Cn%3A21863955031%2Cn%3A21864402031%2Cp_72%3A21909187031&dc&fs=true&language=en&ds=v1%3AaEqB9e39uL4q4gv8EZmR7Pzm9d8qJI8AIhkoKcbetaM&qid=1673219711&rnid=21863955031&ref=sr_nr_n_2'\n",
    "mains=[main_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "titles=[]\n",
    "for x in range(0,5): # the range is the number of pages\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(main_page,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    try:\n",
    "        main2=soup2.find('a',attrs={\"class\": \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"}).get('href')\n",
    "    except:\n",
    "        main2=main_page\n",
    "    if main2 in mains:\n",
    "        append_links(main_page)\n",
    "        break\n",
    "    else:\n",
    "        mains.append('https://www.amazon.eg'+main2)\n",
    "    append_links(main_page)\n",
    "    main_page='https://www.amazon.eg'+main2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mains))\n",
    "print(mains[-5:])\n",
    "print(titles[-5:])\n",
    "print(len(titles))\n",
    "print(links[-5:])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingLinks=[]\n",
    "def appendProduct(link):\n",
    "    '''\n",
    "    this function takes the product link as an argument and returns the title, the price, category, rating, number of ratings, date and the link of the product.\n",
    "    '''\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    title=titles[i]\n",
    "    try:\n",
    "        brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                brand='N/A'\n",
    "    try:\n",
    "        price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                price='N/A'\n",
    "    try:\n",
    "        num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                num_of_ratings='N/A'\n",
    "\n",
    "    today=datetime.date.today()\n",
    "    try:\n",
    "        rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "        if rating=='Previous page':\n",
    "            rating='N/A'\n",
    "        else:\n",
    "            rating=rating\n",
    "    except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "                if rating=='Previous page':\n",
    "                    rating='N/A'\n",
    "                else:\n",
    "                    rating=rating\n",
    "            except AttributeError:\n",
    "                rating='N/A'\n",
    "    if rating == 'N/A' and price == 'N/A':\n",
    "        missingLinks.append(links[i])\n",
    "    link=links[i]\n",
    "    category='food processors'\n",
    "    data=[title,brand,category,price,today,rating, num_of_ratings,link]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over products\n",
    "i=0\n",
    "for link in links:\n",
    "    appendProduct(links[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page='https://www.amazon.eg/s?i=home&bbn=21864088031&rh=n%3A18021933031%2Cn%3A21863792031%2Cn%3A21863827031%2Cn%3A21864088031%2Cn%3A21864618031%2Cp_72%3A21909187031&dc&fs=true&language=en&ds=v1%3ATjaYlZt6%2BQiKZCN9dV8Uiwl84heZskiWDf54xFFmw6E&qid=1673220410&rnid=21864088031&ref=sr_nr_n_2'\n",
    "mains=[main_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links=[]\n",
    "titles=[]\n",
    "for x in range(0,4): # the range is the number of pages\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(main_page,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    try:\n",
    "        main2=soup2.find('a',attrs={\"class\": \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"}).get('href')\n",
    "    except:\n",
    "        main2=main_page\n",
    "    if main2 in mains:\n",
    "        append_links(main_page)\n",
    "        break\n",
    "    else:\n",
    "        mains.append('https://www.amazon.eg'+main2)\n",
    "    append_links(main_page)\n",
    "    main_page='https://www.amazon.eg'+main2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mains))\n",
    "print(mains[-5:])\n",
    "print(titles[-5:])\n",
    "print(len(titles))\n",
    "print(links[-5:])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingLinks=[]\n",
    "def appendProduct(link):\n",
    "    '''\n",
    "    this function takes the product link as an argument and returns the title, the price, category, rating, number of ratings, date and the link of the product.\n",
    "    '''\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    title=titles[i]\n",
    "    try:\n",
    "        brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                brand='N/A'\n",
    "    try:\n",
    "        price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                price='N/A'\n",
    "    try:\n",
    "        num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                num_of_ratings='N/A'\n",
    "\n",
    "    today=datetime.date.today()\n",
    "    try:\n",
    "        rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "        if rating=='Previous page':\n",
    "            rating='N/A'\n",
    "        else:\n",
    "            rating=rating\n",
    "    except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "                if rating=='Previous page':\n",
    "                    rating='N/A'\n",
    "                else:\n",
    "                    rating=rating\n",
    "            except AttributeError:\n",
    "                rating='N/A'\n",
    "    if rating == 'N/A' and price == 'N/A':\n",
    "        missingLinks.append(links[i])\n",
    "    link=links[i]\n",
    "    category='espresso machines'\n",
    "    data=[title,brand,category,price,today,rating, num_of_ratings,link]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over products\n",
    "i=0\n",
    "for link in links:\n",
    "    appendProduct(links[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page='https://www.amazon.eg/s?i=home&bbn=21864401031&rh=n%3A18021933031%2Cn%3A21863792031%2Cn%3A21863827031%2Cn%3A21863955031%2Cn%3A21864401031%2Cp_72%3A21909187031&dc&fs=true&language=en&ds=v1%3AJY%2BuJtvPkJ4eeU3gvw9f3%2B9urALxpLDUzdaJI9hn0nc&qid=1673220767&ref=sr_ex_n_1'\n",
    "mains=[main_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "titles=[]\n",
    "for x in range(0,13): # the range is the number of pages\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(main_page,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    try:\n",
    "        main2=soup2.find('a',attrs={\"class\": \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"}).get('href')\n",
    "    except:\n",
    "        main2=main_page\n",
    "    if main2 in mains:\n",
    "        append_links(main_page)\n",
    "        break\n",
    "    else:\n",
    "        mains.append('https://www.amazon.eg'+main2)\n",
    "    append_links(main_page)\n",
    "    main_page='https://www.amazon.eg'+main2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mains))\n",
    "print(mains[-5:])\n",
    "print(titles[-5:])\n",
    "print(len(titles))\n",
    "print(links[-5:])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingLinks=[]\n",
    "def appendProduct(link):\n",
    "    '''\n",
    "    this function takes the product link as an argument and returns the title, the price, category, rating, number of ratings, date and the link of the product.\n",
    "    '''\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    title=titles[i]\n",
    "    try:\n",
    "        brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                brand='N/A'\n",
    "    try:\n",
    "        price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                price='N/A'\n",
    "    try:\n",
    "        num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                num_of_ratings='N/A'\n",
    "\n",
    "    today=datetime.date.today()\n",
    "    try:\n",
    "        rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "        if rating=='Previous page':\n",
    "            rating='N/A'\n",
    "        else:\n",
    "            rating=rating\n",
    "    except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "                if rating=='Previous page':\n",
    "                    rating='N/A'\n",
    "                else:\n",
    "                    rating=rating\n",
    "            except AttributeError:\n",
    "                rating='N/A'\n",
    "    if rating == 'N/A' and price == 'N/A':\n",
    "        missingLinks.append(links[i])\n",
    "    link=links[i]\n",
    "    category='blenders'\n",
    "    data=[title,brand,category,price,today,rating, num_of_ratings,link]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over products\n",
    "i=0\n",
    "for link in links:\n",
    "    appendProduct(links[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page='https://www.amazon.eg/-/en/s?i=home&bbn=21863955031&rh=n%3A18021933031%2Cn%3A21863792031%2Cn%3A21863827031%2Cn%3A21863955031%2Cn%3A21864400031%2Cp_72%3A21909187031&dc&fs=true&language=en&qid=1673223137&rnid=21863955031&ref=sr_pg_1'\n",
    "mains=[main_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links=[]\n",
    "titles=[]\n",
    "for x in range(0,8): # the range is the number of pages\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(main_page,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    try:\n",
    "        main2=soup2.find('a',attrs={\"class\": \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"}).get('href')\n",
    "    except:\n",
    "        main2=main_page\n",
    "    if main2 in mains:\n",
    "        append_links(main_page)\n",
    "        break\n",
    "    else:\n",
    "        mains.append('https://www.amazon.eg'+main2)\n",
    "    append_links(main_page)\n",
    "    main_page='https://www.amazon.eg'+main2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mains))\n",
    "print(mains[-5:])\n",
    "print(titles[-5:])\n",
    "print(len(titles))\n",
    "print(links[-5:])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingLinks=[]\n",
    "def appendProduct(link):\n",
    "    '''\n",
    "    this function takes the product link as an argument and returns the title, the price, category, rating, number of ratings, date and the link of the product.\n",
    "    '''\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    title=titles[i]\n",
    "    try:\n",
    "        brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                brand='N/A'\n",
    "    try:\n",
    "        price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                price='N/A'\n",
    "    try:\n",
    "        num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                num_of_ratings='N/A'\n",
    "\n",
    "    today=datetime.date.today()\n",
    "    try:\n",
    "        rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "        if rating=='Previous page':\n",
    "            rating='N/A'\n",
    "        else:\n",
    "            rating=rating\n",
    "    except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "                if rating=='Previous page':\n",
    "                    rating='N/A'\n",
    "                else:\n",
    "                    rating=rating\n",
    "            except AttributeError:\n",
    "                rating='N/A'\n",
    "    if rating == 'N/A' and price == 'N/A':\n",
    "        missingLinks.append(links[i])\n",
    "    link=links[i]\n",
    "    category='mixers'\n",
    "    data=[title,brand,category,price,today,rating, num_of_ratings,link]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over products\n",
    "i=0\n",
    "for link in links:\n",
    "    appendProduct(links[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page='https://www.amazon.eg/s?i=home&bbn=21863827031&rh=n%3A18021933031%2Cn%3A21863792031%2Cn%3A21863827031%2Cn%3A21863951031%2Cp_72%3A21909187031&dc&fs=true&language=en&ds=v1%3Amq%2FzHDwBf1%2Bh5tyMIW%2BCd8a%2BkQF5YZbexSfQJI6XQIY&qid=1673223458&rnid=21863827031&ref=sr_nr_n_14'\n",
    "mains=[main_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links=[]\n",
    "titles=[]\n",
    "for x in range(0,12): # the range is the number of pages\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(main_page,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    try:\n",
    "        main2=soup2.find('a',attrs={\"class\": \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"}).get('href')\n",
    "    except:\n",
    "        main2=main_page\n",
    "    if main2 in mains:\n",
    "        append_links(main_page)\n",
    "        break\n",
    "    else:\n",
    "        mains.append('https://www.amazon.eg'+main2)\n",
    "    append_links(main_page)\n",
    "    main_page='https://www.amazon.eg'+main2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mains))\n",
    "print(mains[-5:])\n",
    "print(titles[-5:])\n",
    "print(len(titles))\n",
    "print(links[-5:])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingLinks=[]\n",
    "def appendProduct(link):\n",
    "    '''\n",
    "    this function takes the product link as an argument and returns the title, the price, category, rating, number of ratings, date and the link of the product.\n",
    "    '''\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    title=titles[i]\n",
    "    try:\n",
    "        brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                brand='N/A'\n",
    "    try:\n",
    "        price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                price='N/A'\n",
    "    try:\n",
    "        num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                num_of_ratings='N/A'\n",
    "\n",
    "    today=datetime.date.today()\n",
    "    try:\n",
    "        rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "        if rating=='Previous page':\n",
    "            rating='N/A'\n",
    "        else:\n",
    "            rating=rating\n",
    "    except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "                if rating=='Previous page':\n",
    "                    rating='N/A'\n",
    "                else:\n",
    "                    rating=rating\n",
    "            except AttributeError:\n",
    "                rating='N/A'\n",
    "    if rating == 'N/A' and price == 'N/A':\n",
    "        missingLinks.append(links[i])\n",
    "    link=links[i]\n",
    "    category='toasters'\n",
    "    data=[title,brand,category,price,today,rating, num_of_ratings,link]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over products\n",
    "i=0\n",
    "for link in links:\n",
    "    appendProduct(links[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page='https://www.amazon.eg/s?i=home&bbn=21863961031&rh=n%3A18021933031%2Cn%3A21863792031%2Cn%3A21863827031%2Cn%3A21863961031%2Cn%3A21864412031%2Cp_72%3A21909187031&dc&fs=true&language=en&ds=v1%3A9oO9DbQer7rzESCMmhf2GgkjaTRWsxAw%2B%2F1k4S7BqEk&qid=1673224930&rnid=21863961031&ref=sr_nr_n_1'\n",
    "mains=[main_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links=[]\n",
    "titles=[]\n",
    "for x in range(0,8): # the range is the number of pages\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(main_page,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    try:\n",
    "        main2=soup2.find('a',attrs={\"class\": \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"}).get('href')\n",
    "    except:\n",
    "        main2=main_page\n",
    "    if main2 in mains:\n",
    "        append_links(main_page)\n",
    "        break\n",
    "    else:\n",
    "        mains.append('https://www.amazon.eg'+main2)\n",
    "    append_links(main_page)\n",
    "    main_page='https://www.amazon.eg'+main2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mains))\n",
    "print(mains[-5:])\n",
    "print(titles[-5:])\n",
    "print(len(titles))\n",
    "print(links[-5:])\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingLinks=[]\n",
    "def appendProduct(link):\n",
    "    '''\n",
    "    this function takes the product link as an argument and returns the title, the price, category, rating, number of ratings, date and the link of the product.\n",
    "    '''\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    title=titles[i]\n",
    "    try:\n",
    "        brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                brand=soup2.find(\"a\", {\"id\": \"bylineInfo\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                brand='N/A'\n",
    "    try:\n",
    "        price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                price=soup2.find(\"span\", {\"class\": \"a-price-whole\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                price='N/A'\n",
    "    try:\n",
    "        num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            link=links[i]\n",
    "            page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "            soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "            soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "            num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                num_of_ratings=soup2.find(\"span\", {\"id\": \"acrCustomerReviewText\"}).get_text().strip()\n",
    "            except AttributeError:\n",
    "                num_of_ratings='N/A'\n",
    "\n",
    "    today=datetime.date.today()\n",
    "    try:\n",
    "        rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "        if rating=='Previous page':\n",
    "            rating='N/A'\n",
    "        else:\n",
    "            rating=rating\n",
    "    except AttributeError:\n",
    "            try:\n",
    "                link=links[i]\n",
    "                page = requests.get(link,headers=headers,verify=False, timeout=30)\n",
    "                soup1 = BeautifulSoup(page.content,'html.parser')\n",
    "                soup2 = BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "                rating=soup2.find(\"span\", {\"class\": \"a-icon-alt\"}).get_text().strip()\n",
    "                if rating=='Previous page':\n",
    "                    rating='N/A'\n",
    "                else:\n",
    "                    rating=rating\n",
    "            except AttributeError:\n",
    "                rating='N/A'\n",
    "    if rating == 'N/A' and price == 'N/A':\n",
    "        missingLinks.append(links[i])\n",
    "    link=links[i]\n",
    "    category='electric kettles'\n",
    "    data=[title,brand,category,price,today,rating, num_of_ratings,link]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over products\n",
    "i=0\n",
    "for link in links:\n",
    "    appendProduct(links[i])\n",
    "    i+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this part we clean the amazon data we scraped earlier and scrape the missing values if necessary to make the data ready for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('amazonKitchenFullData.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there're about 20 missing values in the brand column, 26 in the price, 4 in rating and 5 in number of ratings, we need to scrape that data as it's almost about 10% of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.link.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_info=df.query('brand.isnull() | price.isnull() | rating.isnull() | num_of_ratings.isnull()')\n",
    "df_missing_info.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "missing_links=list(df_missing_info.link)\n",
    "len(missing_links)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "filename='amazonMissingItems.csv'\n",
    "header=['title','brand','category','price','date','rating', 'num_of_ratings','link']\n",
    "\n",
    "with open(filename,'w',newline='',encoding='UTF8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def append_missing(link):\n",
    "    link=missing_links[i]\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    title = soup2.find(id='productTitle')\n",
    "    brand = soup2.find('a',attrs={\"class\": \"a-link-normal\", \"id\":\"bylineInfo\"})\n",
    "    price = soup2.find(\"span\", {\"id\": \"tp_price_block_total_price_ww\"})\n",
    "    today = datetime.date.today()\n",
    "    rating = soup2.find('span',attrs={\"class\": \"a-icon-alt\"})\n",
    "    num_of_ratings = soup2.find('span',attrs={\"id\":\"acrCustomerReviewText\"})\n",
    "    category = 'N/A'\n",
    "    data=[title,brand,category,price,today,rating, num_of_ratings,link]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range(0,33):\n",
    "    append_missing(missing_links)\n",
    "    i+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_info=pd.read_csv('amazonMissingItems.csv')\n",
    "df_missing_info.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there're 13 missing prices in the entire dataset we need to delete them and then add the new info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_info = df_missing_info[df_missing_info['price'].notna()]\n",
    "df_missing_info.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a db with sql to join the tables and get the full info for the 20 fixed rows and delete the 33 rows with missing data to get a total of 582 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = sqlite3.connect('amazonKitchen.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('fullAmazonData', cnn, index=False)\n",
    "df_missing_info.to_sql('missingAmazonData', cnn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql sqlite:///amazonKitchen.db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's easier to join the tables on sql so we use it to join the two tables to correctly assign the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_info=pd.read_sql('''SELECT f.title, m.brand, f.category, m.price, m.date, m.rating, m.num_of_ratings, m.link\n",
    " FROM missingAmazonData m\n",
    " JOIN fullAmazonData f\n",
    " on m.link=f.link\n",
    " ''', con=cnn)\n",
    "df_missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,df_missing_info], ignore_index=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['brand'].notna()]\n",
    "df = df[df['price'].notna()]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('amazonKitchen.csv', index=False)\n",
    "df.to_sql('amazonKitchen', cnn, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('amazonKitchen.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now after we made our data complete, we need to remove the html tags from our columns and make the numeric columns become suitable for numeric analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(string):\n",
    "    result = re.sub('<.*?>','',string)\n",
    "    return result\n",
    "df['title']=df['title'].apply(lambda cw : remove_tags(cw))\n",
    "df['brand']=df['brand'].apply(lambda cw : remove_tags(cw))\n",
    "df['price']=df['price'].apply(lambda cw : remove_tags(cw))\n",
    "df['rating']=df['rating'].apply(lambda cw : remove_tags(cw))\n",
    "df['num_of_ratings']=df['num_of_ratings'].apply(lambda cw : remove_tags(cw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].str.replace('\\n', '', regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace('\\n', '', regex=True).str.strip()\n",
    "df['price'] = df['price'].str.replace('\\n', '', regex=True).str.strip()\n",
    "df['rating'] = df['rating'].str.replace('Posted ', '', regex=True).str.strip()\n",
    "df['num_of_ratings'] = df['num_of_ratings'].str.replace(' -', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after checking on excel, there're some brands that are typed more than one way we need to edit them in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brand'] = df['brand'].str.replace('Brand: ', '', regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace('العلامة التجارية: ', '', regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace('&amp;', '&', regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace('Phlips', 'PHILIPS', regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace('Philips Domestic Appliances', 'PHILIPS', regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace('Philips Kitchen Appliances', 'PHILIPS', regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace('MediaTech', 'Media Tech', regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace(\"De'Longhi\", \"DeLonghi\", regex=True).str.strip()\n",
    "df['brand'] = df['brand'].str.replace(\"Touch El Zenouki\", \"Touch Elzenouki\", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_of_ratings'] = df['num_of_ratings'].str.replace('ratings', '', regex=True).str.strip()\n",
    "df['num_of_ratings'] = df['num_of_ratings'].str.replace('rating', '', regex=True).str.strip()\n",
    "df['num_of_ratings'] = df['num_of_ratings'].str.replace('تقييم', '', regex=True).str.strip()\n",
    "df['num_of_ratings'] = df['num_of_ratings'].str.replace(',', '', regex=True).str.strip()\n",
    "df['num_of_ratings'] = pd.to_numeric(df['num_of_ratings'])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].str.replace(' out of 5 stars', '', regex=True).str.strip()\n",
    "df['rating'] = df['rating'].str.replace('من 5 نجوم', '', regex=True).str.strip()\n",
    "df['rating'] = pd.to_numeric(df['rating'])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['price'].str.replace('EGP', '', regex=True).str.strip()\n",
    "df['price'] = df['price'].str.replace(',', '', regex=True).str.strip()\n",
    "df['price'] = df['price'].str.replace(' ', '', regex=True).str.strip()\n",
    "df['price'] = df['price'].str.replace('.\\u200e', '', regex=True).str.strip()\n",
    "df['price'] = df['price'].str[:6]\n",
    "df['price'] = pd.to_numeric(df['price'])\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], dayfirst=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after deleting all the data and cleaning it we save the clean dataset to the db and a scv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('amazonKitchenCleaned.csv', index=False)\n",
    "df.to_sql('amazonKitchenCleaned', cnn, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the main questions**\n",
    "- who are our main brand competitors either overall or in each category?\n",
    "- what are the most succefull products in each category and what are their charactaristics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('amazonKitchenCleaned.csv', index_col=False)\n",
    "cnn = sqlite3.connect('amazonKitchen.db')\n",
    "%load_ext sql\n",
    "%sql sqlite:///amazonKitchen.db\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1** who are our main brand competitors either overall or in each category? <br>\n",
    "to answer this question we type a script to find the best 35 companies in the number of ratings (as it reflects the number of sales) and the count of its products in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['category','brand']).agg(mean_num_of_ratings=('num_of_ratings','mean'), \n",
    "sum_num_of_ratings=('num_of_ratings','sum'), \n",
    "count_of_links=('link','count')).nlargest(columns=['sum_num_of_ratings', 'count_of_links'],n=35, keep='all').sort_values(by=['category', 'sum_num_of_ratings', 'count_of_links'], \n",
    "ascending=[True,False, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can find that Black & Decker appeared in 3 categories as a main competetor with a variety of products so we should consider them.\n",
    "- Braun also appeared in more than one category, Philips has 17 air fryers so it has many products in the air fryers market.\n",
    "- in some categories it seems like there's a brand that has a great number of products and has great number of ratings with a great difference than the next compitetors such as: Bosch in the mixers market, Delonghi in the espresso machines, and braun in the blenders market.\n",
    "- Ninja and hario had one product that got them in the list for the top compitetors in the air fryers and electric kettles markets so we should study these products carefully."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2** who are our main brand competitors either overall or in each category? <br>\n",
    "to answer this question we need to see the products with the most number of ratings in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.boxplot(data=df, x=\"category\", y=\"num_of_ratings\", hue=\"category\", palette=\"dark\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the outliers show the products with the most number of ratings which reflects the most number of sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantile:\n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return x.quantile(self.q)\n",
    "        # Or using numpy\n",
    "        # return np.quantile(x.dropna(), self.q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['category']).agg(mean_num_of_ratings=('num_of_ratings','mean'), median_num_of_ratings=('num_of_ratings','median'), quantile_75=('num_of_ratings',Quantile(0.75)), quantile_95=('num_of_ratings',Quantile(0.95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products=df.loc[(df['num_of_ratings'] >= 407.80) & (df['category'] == 'air fryers')]\n",
    "df_top_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products=df_top_products.append([df.loc[(df['num_of_ratings'] >= 1534.75) & (df['category'] == 'blenders')], \n",
    "df.loc[(df['num_of_ratings'] >= 198.15) & (df['category'] == 'electric kettles')],\n",
    "df.loc[(df['num_of_ratings'] >= 3748.15) & (df['category'] == 'espresso machines')],\n",
    "df.loc[(df['num_of_ratings'] >= 1432.00) & (df['category'] == 'food processors')],\n",
    "df.loc[(df['num_of_ratings'] >= 1150.00) & (df['category'] == 'mixers')],\n",
    "df.loc[(df['num_of_ratings'] >= 1373.50) & (df['category'] == 'toasters')]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there're 33 products that has a number of ratings more than the other 95% products in their category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20,10))\n",
    "sns.boxplot(data=df_top_products, x=\"category\", y=\"num_of_ratings\", hue=\"category\", palette=\"dark\", ax=axes[0])\n",
    "sns.boxplot(data=df_top_products, x=\"category\", y=\"price\", hue=\"category\", palette=\"dark\", ax=axes[1])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see that the air fryers market has the highest prices among all the categories as it varies between around 8000 and 12000 EGP.\n",
    "- the espresso machines market comes in the second place with a little variety and the prices are around 7000 EGP.\n",
    "- after that the other markets are close to each other around 1000 and 3500 EGP prices except for the food processors with a range between 6000 and 2000 EGP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products.link[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products=list(df_top_products.link)\n",
    "len(top_products)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we scrape the description for each product in our list so that we can know more about each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" filename='amazonTopProducts.csv'\n",
    "header=['link','description'] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" with open(filename,'w',newline='',encoding='UTF8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(header) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def append_top(link):\n",
    "    link=top_products[i]\n",
    "    headers= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36\"}\n",
    "    page=requests.get(link,headers=headers,cookies={'lc-acbeg':'en_AE'},timeout=30,verify=False)\n",
    "    soup1= BeautifulSoup(page.content,'html.parser')\n",
    "    soup2=BeautifulSoup(soup1.prettify(),'html.parser')\n",
    "    description = soup2.find(id='productDescription_feature_div')\n",
    "    data=[link,description]\n",
    "    with open(filename,'a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" for i in range(0,33):\n",
    "    append_top(top_products)\n",
    "    i+=1 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products_description=pd.read_csv('amazonTopProducts.csv')\n",
    "df_top_products_description.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(string):\n",
    "    result = re.sub('<.*?>','',string)\n",
    "    return result\n",
    "df_top_products_description['description']=df_top_products_description['description'].apply(lambda cw : remove_tags(cw))\n",
    "df_top_products_description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products_description['description'] = df_top_products_description['description'].str.replace('\\n', '', regex=True).str.strip()\n",
    "df_top_products_description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products_description['description'] = df_top_products_description['description'].apply(lambda st: st[st.find(\"Product \"):st.find(\"#productDescription\")])\n",
    "df_top_products_description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products_description.link[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products_description.description[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_top_products.to_sql('amazonTopProducts', cnn, index=False)\n",
    "df_top_products_description.to_sql('amazonTopProductsDescription', cnn, index=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products=pd.read_sql('''SELECT t.title, t.brand, t.category, t.price, t.date, t.rating, t.num_of_ratings, t.link, d.description\n",
    " FROM amazonTopProducts t\n",
    " JOIN amazonTopProductsDescription d\n",
    " on t.link=d.link\n",
    " ''', con=cnn)\n",
    "df_top_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_top_products.to_sql('amazonFinalTopProducts', cnn, index=False)\n",
    "df_top_products.to_csv('amazonFinalTopProducts.csv', index=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_products=pd.read_csv('amazonFinalTopProducts.csv')\n",
    "df_top_products"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have a dataset ready for the best products with their description that we can study to get an idea of what an ideal product is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the project used beautifulsoup to scrape the amazon website in egypt for 7 categories in the kitchen appliances market.\n",
    "- after the scraping we used SQL and pandas to clean the dataset and get ready for the analysis.\n",
    "- the analysis was performed using seaborn, pandas as SQL to answer the business questions and the results are:\n",
    "    - We concluded that Black & Decker appeared in 3 categories as a main competetor with a variety of products so we should consider them.\n",
    "    - Braun also appeared in more than one category, Philips has 17 air fryers so it has many products in the air fryers market.\n",
    "    - in some categories it seems like there's a brand that has a great number of products and has great number of ratings with a great difference than the next compitetors such as: Bosch in the mixers market, Delonghi in the espresso machines, and braun in the blenders market.\n",
    "    -  Ninja and hario had one product that got them in the list for the top compitetors in the air fryers and electric kettles markets so we should study these products carefully.\n",
    "    - We can see that the air fryers market has the highest prices among all the categories as it varies between around 8000 and 12000 EGP.\n",
    "    - the espresso machines market comes in the second place with a little variety and the prices are around 7000 EGP.\n",
    "    - after that the other markets are close to each other around 1000 and 3500 EGP prices except for the food processors with a range between 6000 and 2000 EGP.\n",
    "    - the top products were saved in a seperate file with their discription and we can track their performance overtime with append_product function to track the ratings and the price."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0472cc80d26e937026be4ad0438e16d3b81a3ce8ea43694b83c33cd89679b03a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
